{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Project Description\n",
      "\n",
      "User requires a system that finds fundamental data on a stock from different sources, particularly from EDGAR fillings. \n",
      "\n",
      "\n",
      "##User Requirements: \n",
      "\n",
      "- [8:44:32 PM]  J: just a quick q\n",
      "- [8:44:41 PM]  J: how difficult would it be, to create a bot, that scans edgar filings\n",
      "- [8:44:52 PM]  J: for the words \"shares\" \"warrants\" \"exercised\" etc.\n",
      "- [8:45:38 PM]  J: 8-k and 10-Q filings with the sec\n",
      "- [8:45:51 PM]  J: so lets say i wake up, i see stock XYZ is up 25%\n",
      "- [8:45:54 PM]  J: i say, oh wow nice, good news\n",
      "- [8:46:02 PM]  J: i input stock name XYZ into pablo bot\n",
      "- [8:46:04 PM]  J: edgar bot tells me:\n",
      "- [8:46:15 PM]  J: 1,900,000 warrants come exerciseable today (or tomorrow)\n",
      "- [8:46:24 PM]  J: float has increased marginally last month\n",
      "- [8:46:29 PM]  J: or blah blah blah blah\n",
      "- [8:46:47 PM]  J: basically ive gotten fucking raped this year for at least 50,000$ buying stocks with news\n",
      "- [8:47:06 PM]  J: and had i read the filings, i'd have known that there was a group of inviduals exercising, or dumping stock right around the news\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Ideas\n",
      "\n",
      "mining EDGAR for tender offers:\n",
      "\n",
      "   - http://www.jot.fm/issues/issue_2008_09/column2/\n",
      "   - http://accretioninfinity.wordpress.com/2013/06/11/installing-hadoop-for-fedora-oracle-linuxsingle-node-cluster/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Resources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### EDGAR\n",
      "EDGAR is a web portal where the SEC makes corporate fillings available \n",
      "\n",
      "http://www.sec.gov/edgar/quickedgar.htm\n",
      "\n",
      "### Notes\n",
      "the SEC uses a company code called CIK which uniquelly identifies companies in their system. \n",
      "We should include this code in our DB, since it is necessary to navigate through the filings in FTP mode. \n",
      "\n",
      "http://www.sec.gov/edgar/NYU/cik.coleft.c\n",
      "\n",
      "They Also support queries based on SIC codes (these are codes that identify industries) \n",
      "example, if you enter 7370 in the SIC box, you'll retrieve all SEC-registered companies that fall into the Standard Industrial Classification known as \"Services-Computer Programming, Data Processing, Etc.\"\n",
      "\n",
      "Searches can also be narrowed down by dates. \n",
      "\n",
      "Types of search supported: \n",
      "\n",
      "\n",
      "- FULL text search:\n",
      "    - goes  over the last 4 years of fillings and attachments. \n",
      "- Current Events Analysis: \n",
      "    - goes over fillings made during the previous week. \n",
      "    - this allows you for example to search for all 10K (anual) or 10Q (quaterly) fillings made in the last week. \n",
      "### FTP access\n",
      "\n",
      "EDGAR supports FTP access\n",
      "\n",
      "http://www.sec.gov/edgar/searchedgar/ftpusers.htm\n",
      "\n",
      "- To use anonymous FTP to access the EDGAR FTP server, use your FTP software to connect to ftp://ftp.sec.gov and log in as user \"anonymous,\" using your electronic mail address as the password.\n",
      "\n",
      "#### Available Index types: \n",
      "\n",
      "Four types of indexes are available:\n",
      "\n",
      "- company \u2014 sorted by company name\n",
      " \n",
      "- form \u2014 sorted by form type\n",
      " \n",
      "- master \u2014 sorted by CIK number\n",
      " \n",
      "- XBRL \u2014 list of submissions containing XBRL financial files, sorted by CIK number; these include Voluntary Filer Program submissions\n",
      "\n",
      "Indexes are located in the following folders according to time period indexed:\n",
      "\n",
      "- /edgar/daily-index \u2014 daily index files through the current year; previous year folders are available through 1994Q3 \n",
      "\n",
      "- /edgar/full-index \u2014 year folders contain quarterly indexes. Full indexes offer a \"bridge\" between Quarterly and Daily indexes, compiling filings for the previous business day through the beginning of the current quarter.\n",
      "\n",
      "\n",
      "- Feed and Oldloads Folders\n",
      "\n",
      "Feed \u2014 The Feed directory contains a tar and gzip archive file (e.g., 20061207.nc.tar.gz; nc stands for non-cooked) for each filing day. Each filing compressed in the tar is a separate filing submission.\n",
      "\n",
      "### RSS feeds for interactive fillings\n",
      "not necessarily immediatly but worth taking a look later to add real time alerts. \n",
      "\n",
      "- http://www.sec.gov/spotlight/xbrl/filings-and-feeds.shtml\n",
      "\n",
      "SEC also supports interactive files with their XBLR format\n",
      "- http://xbrl.sec.gov/\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Filings\n",
      "8-K -> these indicate significant events. \n",
      "everything from a change in CEO to bankrupcy must be disclosed on an 8k. \n",
      "http://en.wikipedia.org/wiki/Form_8-K\n",
      "\n",
      "\n",
      "the items inside the 8k filling have codes that indicate the type of information contained. \n",
      "\n",
      "http://www.sec.gov/answers/form8k.htm\n",
      "\n",
      " - make an interface that allows the user to choose the type of items for the queries. \n",
      " \n",
      " we will use item 3.02 for our examples. \n",
      " \n",
      " \n",
      " The fillings are indexed using the CIK codes, so we need to add those to DB. \n",
      " \n",
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Steps to perform query. \n",
      "### for historical data\n",
      "\n",
      "1. connect to SEC FTP\n",
      "2. get master index\n",
      "- \n",
      "    - build a large index with all historic filings\n",
      "    - this will allow us to use the event profiler on this data. \n",
      "\n",
      "\n",
      "## Further Reading. \n",
      "\n",
      "\n",
      "- http://www.elsevier.pt/en/revistas/the-spanish-review-of-financial-economics-332/artigo/crawling-edgar-90140298\n",
      "\n",
      "- http://stackoverflow.com/questions/13504278/parsing-edgar-filings"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Project Goals\n",
      "\n",
      "\n",
      "we are interested in the Form 8-K filings. these indicate a variety of significant events that are disclosed by companies to the market. \n",
      "http://en.wikipedia.org/wiki/Form_8-K\n",
      "\n",
      "The 8-K filings are divided into Items. These items indicate the type of information contained in the 8-K \n",
      "http://www.sec.gov/answers/form8k.htm\n",
      "\n",
      " \n",
      "For this example we will focus on event item 3.02. On the production version we will allow the user to choose the filings to search. \n",
      " \n",
      "The fillings are indexed using the CIK codes, for this example we will use a fixed CIK code, for the production version we will map the tickers to the CIK code through the company name. \n",
      "\n",
      "\n",
      "\n",
      "## Retrieving the data\n",
      "\n",
      "EDGAR provides FTP access to all of their files. \n",
      "There are over 13 million filings available in their site, so downloading them all is not practical for now. \n",
      "They also provide a \"masters\" file that contains an index of all the filings in the FTP. \n",
      "We will download the complete index from EDGAR and use it to provide search options to the user. \n",
      "This masters file can be updated using daily update files, also provided by the EDGAR FTP site. \n",
      "\n",
      "## Processing the data. \n",
      "\n",
      "The data from EDGAR is not tidy, it comes in HTML format that is not suitable for machine reading out of the box. \n",
      "In order to process the data, we: \n",
      "    1. cleaned the document to remove all HTML tags and leave only the content. \n",
      "    2. used regex to extract the item names contained in each form. \n",
      "    3. used regex to extract the full text of each item. \n",
      "    4. index the full text to the item name into database. \n",
      "## Displaying the results.\n",
      "\n",
      "For our first version we will display the results as formated text so the user can extract the content out of the filings and read only the items that he is interested in reading. \n",
      "\n",
      " "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "CREATE TABLE IF NOT EXISTS edgar_map -- this table holds the index that describes the EDGAR FTP site\n",
      "(\n",
      "    edgar_id SERIAL NOT NULL;\n",
      "    edgar_timestamp TIMESTAMP NOT NULL;\n",
      "    edgar_cik INT NOT NULL; \n",
      "    form_type VARCHAR(20); \n",
      "    edgar_name VARCHAR(200) NOT NULL;\n",
      "    edgar_filename VARCHAR(200) NOT NULL;\n",
      ")"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Imports\n",
      "from ftplib import FTP\n",
      "import re\n",
      "import urllib2\n",
      "from datetime import datetime\n",
      "import pandas as pd\n",
      "import gzip\n",
      "from glob import glob\n",
      "from lxml import etree, html\n",
      "from lxml.html.clean import clean_html\n",
      "import nucleus as nc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Step 1. Get the data from SEC-EDGAR into the DB \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## FUNCTION TO CONNECT TO THE FTP\n",
      "\n",
      "\n",
      "def edgar_connect():\n",
      "    ' This is a generic function to establish a connection with the edgar FTP '\n",
      "    ftp = FTP('ftp.sec.gov')\n",
      "    ftp.login(user='anonymous', passwd='pablo@fractalsoft.biz')\n",
      "    # get to the folder containing all the indexes. \n",
      "    return ftp\n",
      "\n",
      "\n",
      "\n",
      "## FUNCTION THAT GETS THE FILE\n",
      "\n",
      "\n",
      "def get_edgar_map_file(path='/edgar/full-index/2003/QTR4/', filename = 'master.gz', to_db=True):\n",
      "    '''\n",
      "    this function conencts to edgar ftp and retrieves a map file\n",
      "    it loads the file into a pd.DataFrame and checks against the DB to remove any records previously processed. \n",
      "    '''\n",
      "    try:\n",
      "        # \n",
      "        ## get the file from FTP and close connection. \n",
      "        #\n",
      "        ftp = edgar_connect()\n",
      "        ftp.retrbinary(\"RETR \"+path+filename, open(filename, 'wb').write)\n",
      "        ftp.quit()\n",
      "        #\n",
      "        ## read the file into a DF\n",
      "        #\n",
      "        ed= pd.read_table(filename, compression='gzip', header=1, skiprows=[0,1,2,3,4,5,6,9], sep='|')\n",
      "        #\n",
      "        # change the column names on the dataframe. \n",
      "        ed.columns = [\"cik name form_type date filename\".split()]\n",
      "        #\n",
      "        # and change the order of the columns\n",
      "        ed = ed[\"date cik form_type name filename\".split()]\n",
      "        #\n",
      "        ## get the last record from db. \n",
      "        last_db_record = nc.read_db('SELECT edgar_filename FROM edgar_map order by edgar_id desc limit 1;').values[0][0]\n",
      "        #\n",
      "        ## If the last record on DB is duplicate, eliminate all prev records: \n",
      "        if last_db_record in ed.filename.values[-1]:\n",
      "            # if the last record is also the last entry on ed. \n",
      "            return \n",
      "        elif last_db_record in ed.filename.values: \n",
      "            ed = ed[ed[ed.filename == last_db_record].index:]\n",
      "        # write the DF on DB\n",
      "        if to_db: \n",
      "            write_ed(ed)\n",
      "        else: \n",
      "            return ed\n",
      "    except Exception as e:\n",
      "        print e\n",
      "\n",
      "        \n",
      "### FUNCTIONS THAT WRITE THE FILE TO DB IN PARALLEL. \n",
      "def write_ed(ed): \n",
      "    '''\n",
      "    this function generates and calls the parallel function to write into DB. \n",
      "    '''\n",
      "    cpu = nc.cpu_count()\n",
      "    ed_task = [(write_ed_par, (ed, idx, cpu)) for idx in range(cpu)]\n",
      "    nc.multi_process(ed_task)\n",
      "    return \n",
      "\n",
      "def write_ed_par(ed, idx=0, cpu=1): \n",
      "    '''\n",
      "    ed if a dataframe containing entries of the map for the edgar site\n",
      "    '''\n",
      "    sql = str(\"INSERT INTO edgar_map(edgar_timestamp, edgar_cik, \"+\n",
      "              \"form_type, edgar_name, edgar_filename) \"+\n",
      "              \"VALUES(%s, %s, %s, %s, %s);\")\n",
      "    \n",
      "    data = []\n",
      "    #\n",
      "    # split the file in 1 shot\n",
      "    #\n",
      "    data = []\n",
      "    split = len(ed)/cpu\n",
      "    if idx >= cpu-1:\n",
      "        data_file = ed[idx*split:]\n",
      "    else: \n",
      "        data_file = ed[idx*split : (1+idx)*split]\n",
      "    for row in data_file.values: \n",
      "        data += [tuple(row)]\n",
      "    nc.write_db(sql, data)\n",
      "    return  \n",
      "\n",
      "\n",
      "\n",
      "###  FUNCTION THAT GENERATE THE PATH AN FILENAME TO DOWNLOAD THE MAP FOR A FULL DAY OR QUATER\n",
      "\n",
      "def get_edgar_days(last_day, today): \n",
      "    path = '/edgar/daily-index/'\n",
      "    # get the list of idx files. \n",
      "    ftp = edgar_connect()\n",
      "    ftp.cwd(path)\n",
      "    filenames= re.findall('master.[0-9]*.idx', str(ftp.nlst()))\n",
      "    ftp.quit()\n",
      "    #\n",
      "    # Extract the valid dates from the list\n",
      "    file_dates = re.findall('[0-9][0-9]*', str(filenames))\n",
      "    # now get the dates that are present in the filesnames\n",
      "    #\n",
      "    # generate a pandas time range covering all the necessary days\n",
      "    date_range =  pd.date_range(last_day, periods=(today-last_day).days, freq='d')\n",
      "    #\n",
      "    # Loop over the generated dates, and find the ones that are missing. \n",
      "    for i in range(1, len(date_range)):\n",
      "        target_date = date_range[i].strftime(\"%Y%d%m\")\n",
      "        if target_date in file_dates:\n",
      "            get_edgar_map_file(path, filenames[file_dates.index(target_date)])\n",
      "               \n",
      "    \n",
      "    return\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def get_edgar_quaters(last_day):\n",
      "    '''\n",
      "    this function gets the edgar_map, 1 quater at a time\n",
      "    '''\n",
      "    ftp = edgar_connect()\n",
      "    ftp.cwd('/edgar/full-index')\n",
      "    # get a list of the years available for download\n",
      "    years = re.findall('[0-9][0-9][0-9][0-9]', str(ftp.nlst()))\n",
      "    ftp.quit()\n",
      "    \n",
      "    for year in years:\n",
      "        # For the last year already in DB. get only the quaters that are missing. \n",
      "        if int(year) == last_day.year:\n",
      "            ftp = edgar_connect()\n",
      "            ftp.cwd('/edgar/full-index/%s/' %(year))\n",
      "            quater_list = re.findall('QTR[0-9]', str(ftp.nlst()))\n",
      "            ftp.quit()\n",
      "            for quater in quater_list[(last_day.month/3):]: \n",
      "                path = \"/edgar/full-index/%s/%s/\"%(year, quater)\n",
      "                get_edgar_map_file(path, 'master.gz')\n",
      "        # for every other year, get all quaters. \n",
      "        elif int(year) > last_day.year:\n",
      "            ftp = edgar_connect()\n",
      "            ftp.cwd('/edgar/full-index/%s/' %(year))\n",
      "            # get a list of the quaters available for download. \n",
      "            quater_list = re.findall('QTR[0-9]', str(ftp.nlst()))\n",
      "            ftp.quit()\n",
      "            for quater in quater_list: \n",
      "                path = \"/edgar/full-index/%s/%s/\"%(year, quater)\n",
      "                get_edgar_map_file(path, 'master.gz')\n",
      "        else: \n",
      "            print year+\" already in database\"\n",
      "    return\n",
      "#\n",
      "\n",
      "\n",
      "### FUNCTION THAT CHECKS THE LAST DAY ON FILE AND CHOOSES TO DOWNLOAD QUATERS OR DAYS\n",
      "\n",
      "\n",
      "def build_edgar_masters(truncate=False):\n",
      "    '''\n",
      "    This program crawls through the SEC FTP site and gets all the historic master files. \n",
      "    It first reads from the DB to get the last date on record. \n",
      "    \n",
      "    '''\n",
      "    if truncate: nc.truncate_db('edgar_map')\n",
      "    \n",
      "    # check the date of the last masters record in db. \n",
      "    last_day = pd.tslib.Timestamp(nc.read_db(\"SELECT MAX(edgar_timestamp) FROM edgar_map; \").values[0][0])\n",
      "    #\n",
      "    today = pd.Timestamp('today')\n",
      "    #\n",
      "    if (today-last_day).days >=30: \n",
      "        get_edgar_quaters(last_day)\n",
      "    else: \n",
      "        # if less than 30 days are missing\n",
      "        get_edgar_days(last_day, today)\n",
      "    #\n",
      "    return\n",
      "\n",
      "\n",
      "# Build the map on the DB \n",
      "build_edgar_masters()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Step 2. Read the files. \n",
      "\n",
      "\n",
      "\n",
      "The index files name\n",
      "masters/master.YYYYMMDD.z"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_filings(name, cik, verbose=True):\n",
      "    '''\n",
      "    this function gets a list of files from the SEC FTP\n",
      "    '''\n",
      "    # connect to the SEC FTP\n",
      "    ftp = FTP('ftp.sec.gov')\n",
      "    ftp.login(user='anonymous', passwd='pablo@fractalsoft.biz')\n",
      "    # check the filings files already downloaded\n",
      "    existing_files = glob(\"filings/*.html\")\n",
      "    # create a name for the target file\n",
      "    target_name = str(cik)+'-'+name[:-3]+\"html\" \n",
      "    # check that the file is not already created\n",
      "    if \"filings/\"+target_name not in existing_files:\n",
      "        if verbose: print 'processing %s'%(target_name)\n",
      "        print cik\n",
      "        ftp.cwd('/edgar/data/%s/'%(cik))\n",
      "        if verbose: print name[i]\n",
      "        ftp.retrbinary(\"RETR %s\"%(name[i]), open(\"filings/\"+target_name, 'wb').write)\n",
      "    else:\n",
      "        if verbose: print target_name + \" already exists\"\n",
      "    return target_name\n",
      "\n",
      "\n",
      "def html_cleaner(html_file):\n",
      "    '''\n",
      "    this function removes the tags from an HTML leaving only the content\n",
      "    '''\n",
      "    tree = etree.parse(html_file, html.HTMLParser())\n",
      "    tree = clean_html(tree)\n",
      "    clean_text =  tree.getroot().text_content()\n",
      "    # first build a dirty version of the lists, which has newline characters\n",
      "    items_list = re.findall(\"\\n [ ]*Item [0-9][0-9. ]*\",clean_text)\n",
      "    items_list += re.findall(\"\\nItem [0-9][0-9. ]*\",clean_text)\n",
      "    items_list += re.findall(\"\\nITEM [0-9][0-9. ]*\",clean_text)\n",
      "    items_list += re.findall(\"\\n [ ]*ITEM [0-9][0-9. ]*\",clean_text)\n",
      "    # exhibits are included under item 9.01\n",
      "    # use the dirty version to build a list of locations in the file. \n",
      "    locations = []\n",
      "    for item in items_list: \n",
      "        locations += [clean_text.index(item)]\n",
      "    \n",
      "    # add the ending point for Exhibits, where the About Co section start. \n",
      "    locations += [len(clean_text)]\n",
      "    # clean the items list\n",
      "    \n",
      "    name_list = re.findall( 'item [0-9.][0-9. ]*', str(items_list).lower())\n",
      "    filings = {}\n",
      "    for i in range(len(name_list)): \n",
      "        name = name_list[i]\n",
      "        filings[name] = clean_text[locations[i]:locations[i+1]]\n",
      "    \n",
      "    return filings\n",
      "\n",
      "\n",
      "def process_8k(ticker=\"WPCS\", verbose=True):\n",
      "    '''\n",
      "    this function processes 8k fillings. \n",
      "    inputs: \n",
      "        masters: pd DataFrame containing the information of the fillings masters from the SEC EDGAR site. \n",
      "        cik: CIK indentifier for an issuer. \n",
      "    '''\n",
      "    cik = get_cik(ticker)\n",
      "    # get the masters file\n",
      "    masters= masters_to_pd(cik=cik)\n",
      "    # Get the 8K filings. \n",
      "    form_8k = masters[masters[\"Form Type\"]=='8-K']\n",
      "    print form_8k\n",
      "    # initialize the variable to store the output. \n",
      "    forms = pd.DataFrame()\n",
      "    # iterate over the 8-K filings and add them to the output. \n",
      "    for i in range(len(form_8k)): \n",
      "        # get 1 filing. \n",
      "        item = form_8k.iloc[i:i+1]\n",
      "        # get the name of the file\n",
      "        name = re.findall('[0-9]*-[0-9]*-[0-9]*.txt', str(item.Filename))[0]\n",
      "        \n",
      "        # get the items out of the HTML code\n",
      "        details = html_cleaner( 'filings/' + get_filings(name, item.CIK.values[0], verbose))\n",
      "        for key in details.keys():\n",
      "            # add each item to the DF\n",
      "            item[float(re.findall('[0-9]*[0-9.][0-9]*', key)[0])] =details[key]\n",
      "        # add DF to output. \n",
      "        forms= forms.append(item)\n",
      "    return forms\n",
      "\n",
      "def masters_to_pd(cik=None):\n",
      "    '''\n",
      "    this function gets all the 8k forms together into a single dataframe\n",
      "    \n",
      "    '''\n",
      "    masters = pd.DataFrame()\n",
      "    masters_list = glob('masters/*.gz')\n",
      "    for master_file in masters_list:\n",
      "        masters = masters.append(pd.read_table(master_file, compression='gzip', header=1, skiprows=[0,1,2,3,4,5,6,9], sep='|'))\n",
      "    masters.index = masters['Date Filed']\n",
      "    if cik == None:\n",
      "        return masters\n",
      "    else: \n",
      "        return masters[masters.CIK==cik]\n",
      "     \n",
      "def get_cik(ticker):\n",
      "    '''\n",
      "    this function uses yahoo to translate a ticker into a CIK\n",
      "    '''\n",
      "    url = \"http://finance.yahoo.com/q/sec?s=%s+SEC+Filings\"%(ticker)\n",
      "    return int(re.findall('=[0-9]*', str(re.findall('cik=[0-9]*', urllib2.urlopen(url).read())[0]))[0][1:])\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\": \n",
      "    ticker = \"WPCS\"\n",
      "    form_8k = process_8k(ticker)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Index: 141 entries, 2003-10-28 to 2012-03-27\n",
        "Data columns (total 5 columns):\n",
        "CIK             141  non-null values\n",
        "Company Name    141  non-null values\n",
        "Date Filed      141  non-null values\n",
        "Filename        141  non-null values\n",
        "Form Type       141  non-null values\n",
        "dtypes: int64(1), object(4)\n",
        "1086745-0001013762-03-000512.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-03-000682.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-04-000339.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-12-066426.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-12-068228.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-12-068297.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-12-069937.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-04-000430.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-04-000679.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-05-000715.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-10-001722.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-10-001731.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-10-002108.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-10-002243.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-10-002252.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-10-002319.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-07-000524.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-07-000577.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-07-000600.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-05-000312.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-05-001837.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001004878-01-000044.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001004878-01-500005.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001004878-01-500006.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-08-000444.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-08-000546.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-07-000148.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-07-000390.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-02-000072.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-10-000744.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-08-001435.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-08-001539.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-08-001611.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-08-002007.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-06-000791.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-06-000944.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-06-001203.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-06-001296.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-031678.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-08-002165.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-08-002428.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-08-002515.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-08-002619.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-012549.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-015828.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-015925.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-001857.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-001998.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-002003.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-002115.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-002439.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-002497.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-002511.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-002593.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-06-000574.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-06-000597.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-05-000911.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-05-001163.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001004878-00-000029.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001004878-00-000037.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-09-000393.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-09-000451.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-040812.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-041646.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-041720.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-042542.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-042811.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-050625.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-050965.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-051054.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-052373.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-052691.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-07-002051.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-07-002368.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-07-002470.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-09-001317.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-09-001678.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-04-000881.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-06-002502.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-06-001387.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-06-001477.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-06-001506.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-06-001872.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001004878-00-000063.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-040812.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-041646.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-041720.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-042542.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-042811.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-050625.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-050965.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-051054.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-052373.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001144204-13-052691.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-12-000718.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-12-001006.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-12-001403.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-000637.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-000649.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-000738.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-09-002002.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-09-002322.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-02-000117.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-003262.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-003308.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-11-003312.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-07-001343.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-07-001414.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-07-001708.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-02-000153.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-10-002526.html already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1086745-0001013762-10-002568.html already exists"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-532865f9b3d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[0mticker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"WPCS\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m     \u001b[0mform_8k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_8k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-3-532865f9b3d0>\u001b[0m in \u001b[0;36mprocess_8k\u001b[1;34m(ticker, verbose)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;31m# get the items out of the HTML code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mdetails\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtml_cleaner\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'filings/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mget_filings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCIK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;31m# add each item to the DF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-3-532865f9b3d0>\u001b[0m in \u001b[0;36mget_filings\u001b[1;34m(name, cik, verbose)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# connect to the SEC FTP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mftp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFTP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ftp.sec.gov'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mftp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'anonymous'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pablo@fractalsoft.biz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m# check the filings files already downloaded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mexisting_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filings/*.html\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib64/python2.7/ftplib.pyc\u001b[0m in \u001b[0;36mlogin\u001b[1;34m(self, user, passwd, acct)\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0mpasswd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpasswd\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'anonymous@'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendcmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'USER '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'3'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendcmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PASS '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'3'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendcmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ACCT '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0macct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib64/python2.7/ftplib.pyc\u001b[0m in \u001b[0;36msendcmd\u001b[1;34m(self, cmd)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;34m'''Send a command and return the response.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputcmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvoidcmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib64/python2.7/ftplib.pyc\u001b[0m in \u001b[0;36mgetresp\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;31m# Raise various errors if the response indicates an error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetresp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetmultiline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebugging\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'*resp*'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msanitize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlastresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib64/python2.7/ftplib.pyc\u001b[0m in \u001b[0;36mgetmultiline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m                 \u001b[0mnextline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnextline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnextline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib64/python2.7/ftplib.pyc\u001b[0m in \u001b[0;36mgetline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;31m# Raise EOFError if the connection is closed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebugging\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;34m'*get*'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msanitize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib64/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    445\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### production version. \n",
      "in the production version of this software we will read the DB so that the user can choose which filing types to download. \n",
      "e.g. the user could choose to DL only the 8-K reports. \n",
      "\n",
      " - for this version we download all files from 1 stock in the 2013 QTR4. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\": \n",
      "    print form_8k\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'form_8k' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-4-b54e83114be4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mprint\u001b[0m \u001b[0mform_8k\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'form_8k' is not defined"
       ]
      }
     ],
     "prompt_number": 4
    }
   ],
   "metadata": {}
  }
 ]
}